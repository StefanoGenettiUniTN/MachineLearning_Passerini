\chapter{Linear algebra}
\label{cha:linear_algebra}
\defi{\textbf{Group} \label{def:group}\\In abstract algebra, a group is a set $G$ where an operation $*$ is defined. Moreover $*$ satisfies the following properties:
\begin{enumerate}
    \item \textit{associative}: $(a*b)*c=a*(b*c) \forall a,b,c \in G$
    \item \textit{identity element}: $\exists e \in G : a*e=e*a=a \forall a \in G$
    \item \textit{inverse element}: $\forall a \in G$ $\exists a' \in G : a*a'=a'*a=e$
\end{enumerate}
}

\defi{\textbf{Commutative group} \label{def:comm_group}\\
A group $(G,*)$ is \textit{commutative} iff $a*b=b*a$ $\forall a,b \in G$.
}

\defi{\textbf{Vector space} \label{def:vector_space}\\
A \textit{vector space} over the field $\mathcal{R}$ is an algebraic structure composed by a commutative group $(V,+)$, whose elements are called $vectors$, and a function $f:\mathcal{R} \times V \rightarrow V$ called \textit{scalar multiplication} which satisfies the properties listed below. It is common to use $kv$, instead of $f(k,v)$ to represent the scalar multiplication between a scalar $k \in \mathcal{R}$ and a vector $v \in V$.
\begin{itemize}
    \item \textit{distributive over elements} $k(v_1+v_2)=kv_1+kv_2$
    \item \textit{distributive over scalars} $(k_1+k_2)v=k_1v+k_2v$
    \item \textit{associative over scalars} $(k_1k_2)v=k_1(k_2v)$
    \item \textit{identity element} $1v=v$
\end{itemize}
$\forall k,k_1,k_2 \in \mathcal{R}, \forall v, v_1, v_2 \in V$.
}
Vector space examples are: the set of two-dimensional vectors $V^2$, the set of three-dimensional vectors $V^3$, the matrix space $M_{m,n}(\mathcal{R})$.

\defi{\textbf{Subspace} \label{def:subspace}\\
Given a vector space $V$ defined over the field $R$, a non-empty set $W \subseteq V$ is a \textit{subspace} of $V$ if it is close w.r.t. the sum of vectors and the scalar product:
\begin{equation*}
    \forall w,w'\in W, \forall k \in K, w+w' \in W \wedge kw \in W
\end{equation*}
Each subspace is in turn a vector space with respect to the operations defined in $V$.
}

\defi{\textbf{Linear combination} \label{def:linear_combination}\\
Given a vector $v \in V$, $v$ is a linear combination of the vectors $v_1,...,v_k \in V$ with coefficients $c_i \in \mathcal{R}$ if:
\begin{equation*}
    v = c_1v_1+...+c_kv_k = \sum_{i=1}^K c_iv_i
\end{equation*}
}

\defi{\textbf{Span} \label{def:span}\\
The span of vectors $v_1,...,v_k$ is defined as the set of their linear combinations:
\begin{equation*}
    \{\sum_{i=1}^K c_iv_i, c_i \in \mathcal{R}\} = <v_1,...,v_k>
\end{equation*}
The span is a \textit{vector subspace} (also known as \textit{linear subspace}) of $V$. In particular the span $<v_1,...,v_k>$ is the subset generated by $v_1,...,v_k$.
}

\defi{\textbf{Linear independency and linear dependency} \label{def:indipendency1}\\
A set of vectors $v_1,...,v_k$ is \textit{linearly independent} if the null vector can be written as a linear combination of the elements $v_i$ of the set

\begin{equation*}
    a_1v_1+a_2v_2+...+a_kv_k=0
\end{equation*}

if and only if all the coefficients $a_1,...,a_k$ are null. Intuitively, a set of vectors $v_1,...,v_k$ is linearly independent if none of them can be written as a linear combination of the others.
\\
A set of vectors $v_1,...,v_k$ is \textit{linearly dependent} is it is not linearly independent. As a consequence, a set of vectors $v_1,...,v_k$ is linearly dependent if there exist some not-null scalars $a_1,...,a_k$ such that:
\begin{equation*}
    a_1v_1+a_2v_2+...+a_kv_k=0
\end{equation*}
}

\defi{\textbf{Basis} \label{def:basis1}\\
A set of vectors $\mathcal{B}=\{v_1,...,v_k\}$ is a \textit{basis} for $V$ if any element in $V$ can be uniquely written as a linear combination of vectors $v_i$. A necessary condition is that vectors $v_i$ are linearly independent. All bases of $V$ have the same number of elements, called the \textit{dimension} of the vector space.
}
\defi{\textbf{Linear maps} \label{def:linear_maps}\\
Given two vector spaces $V$ and $V'$, a function $f: V \rightarrow V'$ is a \textit{linear map} if $\forall a_1, a_2 \in \mathcal{R}, v_1, v_2 \in V$:
\begin{equation*}
    f(a_1v_1+a_2v_2)=a_1f(v_1)+a_2f(v_2)
\end{equation*}
}

A linear map between two finite-dimensional spaces $V$, $V'$ of dimensions $n$,$m$ can always be written as a matrix $A$. Consider a linear map $T: V \rightarrow V'$ and two sets $\mathcal{B}=\{u_1,...,u_n\}$, $\mathcal{C}=\{v_1,...,v_m\}$ which are basis of $V$ and $V'$ respectively.

Consider also two isomorphisms $T_{\mathcal{B}}: V \rightarrow {\mathcal{R}}^{n}$, $T_{\mathcal{C}}: V' \rightarrow {\mathcal{R}}^{m}$ which associate to each vector its coordinates with respect to a given base. These two isomorphisms uniquely identify a matrix $A \in M_{m,n}(\mathcal{R})$. The $n$ columns of $A$ are the vectors $T_{\mathcal{C}}(T(u_j))$ obtained from the coordinates of the images $T(u_1),...,T(u_n)$, with respect to basis $\mathcal{C}$.

For any $x \in V$ we have:
\begin{itemize}
    \item Each element $x \in V$ can be written as a linear combination of the elements of the basis $\mathcal{B}$.
\end{itemize}
\begin{equation*}
    f(x) = f(\sum_{i=1}^n \lambda_i u_i) = \sum_{i=1}^n \lambda_i f(u_i)
\end{equation*}
\begin{itemize}
    \item Given that the $n$ columns of $A$ are the vectors $T_{\mathcal{C}}(T(u_j))$ obtained from the coordinates of the images $T(u_1),...,T(u_n)$, with respect to basis $\mathcal{C}$.
\end{itemize}
\begin{equation*}
    f(u_i) = \sum_{j=1}^m a_{ji}v_j
\end{equation*}
\begin{itemize}
    \item Putting all together we notice that actually the image of $f(x)$ can be written as a linear combination of the elements of the basis $\mathcal{C}$. As a consequence, $f(x)$ belongs to the vector space $V'$.
\end{itemize}
\begin{equation*}
    f(x) = \sum_{i=1}^n \lambda_i f(u_i) = \sum_{i=1}^n \sum_{j=1}^m \lambda_i a_{ji} v_j = \sum_{j=1}^m (\sum_{i=1}^n \lambda_i a_{ji}) v_j = \sum_{j=1}^m \mu_j v_j
\end{equation*}

The matrix $A \in M_{mn}(\mathcal{R})$ defined above is called the matrix associated with the linear map $T$ with respect to the basis $\mathcal{B}$, $\mathcal{C}$. $A$ is typically indicated with the symbol $M^\mathcal{C}_\mathcal{B}(T)$. If $V=V'$ and $\mathcal{B}=\mathcal{C}$, then we write $M_\mathcal{B}(T)$. If $V=\mathcal{R}^n$, $V'=\mathcal{R}^m$ and we consider the canonical basis of the two vector spaces, then we simply write $M(T)$.


Consider a linear map $T$ and a matrix $A=M^\mathcal{C}_\mathcal{B}(T)$ defined as described above. Consider also a vector $v \in V$ and a vector $x$. This latter is the column vector of the coordinates $x_1,...,x_n$ of $v$ with respect to the basis $\mathcal{B}$. Given that, the image of $T(v)$ has a corresponding vector of coordinates with respect to basis $\mathcal{C}$ which can be calculated by the matrix multiplication $Ax$. We propose an example in Figure~\ref{fig:linear_function_matrix}.
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/Esempio matrice associata a una funzione lineare.pdf}
    \caption{Example of a computation of the matrix associated with a linear function}
    \label{fig:linear_function_matrix}
\end{figure}

\defi{\textbf{Matrix of basis transformation} \label{def:basis_transformation}\\
Consider two basis of the vector space $V$: $\mathcal{B}=\{v_1,...,v_n\}$ and $\mathcal{B}'=\{v'_1,...,v'_n\}$. The \textit{transition matrix} (or \textit{matrix of basis transformation}) from $\mathcal{B}$ to $\mathcal{B}'$ is the invertible matrix $P=[p_{ij}]$, with order $n$, whose columns are the coordinates of the vectors $v_j$ of the basis $\mathcal{B}$ with respect to the basis $\mathcal{B}'$.
\begin{equation*}
    v_j = \sum_{i} p_{ij}v'_i \quad (j = 1,...,n)
\end{equation*}
}

\textbf{Remark:} The matrix $P$ is the matrix associated with the identity function with respect to the basis $\mathcal{B}$ and $\mathcal{B}'$ : $P = M^{\mathcal{B}'}_{\mathcal{B}}(id_V)$. From this observation we can derive the following consequence. If the vector $v \in V$ has coordinates $x_1,...,x_n$ with respect to basis $\mathcal{B}$, and coordinates $x'_1,...,x'_n$ with respect to basis $\mathcal{B}'$, then:
\begin{equation}
    x' = Px
\end{equation}
where $P$ is the matrix of basis transformation from $\mathcal{B}$ to $\mathcal{B}'$. We propose an example of basis transformation in $\mathcal{R}^2$ in Figure \ref{fig:basis_transformation}.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/Esempio matrice cambiamento di base.pdf}
    \caption{2D example of basis transformation.}
    \label{fig:basis_transformation}
\end{figure}

\defi{\textbf{Transpose} \label{def:transpose}\\
Matrix obtained exchanging rows with columns (indicated with $M^T$).\\Given two matrices $M$,$N$ and their transposes $M^T$, $N^T$, the following relevant property holds:
\begin{equation*}
    (MN)^T = N^TM^T
\end{equation*}
}

\defi{\textbf{Trace} \label{def:trace}\\
Sum of diagonal elements of a matrix.
\begin{equation*}
    tr(M) = \sum_{i=1}^n M_{ii}
\end{equation*}
}

\defi{\textbf{Inverse} \label{def:inverse}\\
The matrix which multiplied with the original matrix gives the identity.
\begin{equation*}
    MM^{-1}=I
\end{equation*}
}

\defi{\textbf{Rank} \label{def:rank}\\
The rank of a $n \times m$ matrix is the dimension of the space spanned by its columns. Consider a matrix $A$ $m \times n$. In the matrix there are $n$ columns $A^1,...,A^n$. It is possible to demonstrate that the column space $<A^1,...,A^n>$ has dimension equal to the rank of the matrix $A$.
\begin{equation*}
    rg(A) = \mathit{dim}(<A^1,...,A^n>)
\end{equation*}
In particular the columns of $A$ are linearly independent if and only if $rg(A)=n$.
}

\section{Matrix derivatives}
In this section, there are reported some useful properties of matrix calculus. It takes a lot of time to write the formulas in \LaTeX     properly, as a result we illustrate them in Figure \ref{fig:matrix_derivatives_properties}.


\begin{figure}
    \centering
    \includegraphics[scale=0.35]{images/matrix_derivatives.png}
    \caption{Matrix derivatives properties}
    \label{fig:matrix_derivatives_properties}
\end{figure}


\textbf{Note:} Results are column vectors. Transpose them if row vectors are needed instead.

\section{Metric structure}
\defi{\textbf{Norm} \label{def:norm}\\
The \textit{norm} defined in $\mathcal{R}^n$ is a function $||\cdot||:\mathcal{X} \rightarrow \mathcal{R}_0^+$ which associates to the vector $x$ the non-negative real number:
\begin{equation}
    ||x|| = \sqrt{x \cdot x} = \sqrt{x_1^2 + ... + x_n^2}
\end{equation}
The norm has the following properties:
\begin{itemize}
    \item $||x+y|| \leq ||x|| + ||y||$
    \item $||\lambda x|| = |\lambda| ||x||$
    \item $||x||>0$ if $x \neq 0$
\end{itemize}
where $x,y \in \mathcal{X}$, $\lambda \in \mathcal{R}$
}

\defi{\textbf{Metric} \label{def:metric}\\
A norm defines a \textit{metric} $d: \mathcal{X} \times \mathcal{X} \rightarrow \mathcal{R}_0^+$ such that:
\begin{equation}
    d(x,y) = ||x-y||
\end{equation}
This metric is commonly referred to as $distance$. This latter is a function which associates to each couple of vectors $x,y \in \mathcal{R}^n$ the real number $d(x,y)$.
}

\textbf{Remark:} The concept of norm is stronger than that of metric: not any metric gives rise to a norm.\\

\defi{\textbf{Unit vector} \label{def:unit_vector}\\
A \textit{unit vector} is a vector $v \in \mathcal{R}^n$ such that:
\begin{equation*}
    ||v|| = 1
\end{equation*}
}

\textbf{Remark:} each vector $v \neq \pmb{0}$ can be normalized:
\begin{equation*}
    v' = \frac{v}{||v||}
\end{equation*}
where $v'$ is a unit vector.

\section{Dot product}
\defi{\textbf{Bilinear form} \label{def:bilinear_form}\\
A function $Q: \mathcal{X} \times \mathcal{X} \rightarrow \mathcal{R}$ on a vector space $X$ is a \textit{bilinear form} if it is linear in each argument separately:
\begin{equation*}
    Q(\lambda x + \mu y, z) = \lambda Q(x,z) + \mu Q(y,z)
\end{equation*}

\begin{equation*}
    Q(x, \lambda y + \mu z) = \lambda Q(x,y) + \mu Q(x,z)
\end{equation*}
where $x,y,z \in \mathcal{X}$, $\lambda, \mu \in \mathcal{R}$.

A bilinear form is \textit{symmetric} if for all $x,y \in \mathcal{X}$:
\begin{equation*}
    Q(x,y) = Q(y,x)
\end{equation*}

}

\defi{\textbf{Dot product} \label{def:dot_product}\\
The \textit{dot product} on $\mathcal{R}^n$ is a function $<\cdot,\cdot>: \mathcal{X} \times \mathcal{X} \rightarrow \mathcal{R}$ which associate to a couple of vectors $x,y \in \mathcal{R}^n$ the real number:
\begin{equation}
    <x,y> = x \cdot y = \sum_{i=1}^n x_iy_i = x^T y
\end{equation}
}

The dot product has the following properties:
\begin{enumerate}
    \item \textit{symmetric}: $x \cdot y = y \cdot x$ for all $x,y \in \mathcal{R}^n$
    
    \item \textit{bilinear}: $(\alpha x + \beta y) \cdot z = \alpha (x \cdot z) + \beta (y \cdot z)$ and $x \cdot (\alpha y + \beta z) = \alpha (x \cdot y) + \beta (x \cdot z)$, for all $x,y,z \in \mathcal{R}^n$ and $\alpha, \beta \in \mathcal{R}$
    
    \item \textit{symmetric bilinear}: this property follows from the previous ones
    
    \item \textit{positive semi-definite}: $x \cdot x \geq 0$ for all $x \in \mathcal{R}^n$
    
    \item The dot product could also be \textit{positive definite} if it satisfies: $x \cdot x = 0$ iff $x=0$ for all $x \in \mathcal{R}^n$
\end{enumerate}

Any dot product defines a corresponding \textit{norm} via:
\begin{equation*}
    ||x|| = \sqrt{<x,x>}
\end{equation*}

\begin{itemize}
    \item The angle $\theta$ between two vectors is defined as:
\end{itemize}
\begin{equation}
    \cos{\theta} = \frac{<x,z>}{||x||||z||}
\end{equation}

\begin{itemize}
    \item Two vectors are \textit{orthogonal} if $<x,y>=0$.
    
    \item A set of vectors $\{x_1,...,x_n\}$ is orthonormal if all vectors $x_i$ in the set are mutually orthogonal and are all unit vectors:
\end{itemize}
\begin{equation*}
    <x_i, x_j> = \delta_{ij}
\end{equation*}
\hspace{7mm} where $\delta_{ij}=1$ if $i=j$, $0$ otherwise.

\section{Eigenvalues and eigenvectors}
\label{sec:eigenvalues_eigenvectors}

\defi{\textbf{Eigenvalues and eigenvectors} \label{def:eigen}\\
Given a $n \times n$ matrix $M$, the real value $\lambda$ and (non-zero) vector $x$ are an \textit{eigenvalue} and corresponding \textit{eigenvector} of $M$ if:
\begin{equation}
    Mx=\lambda x
\end{equation}
}

\textbf{Properties:}
\begin{itemize}
    \item A $n \times n$ matrix has $n$ eigenvalues
    \item A $n \times n$ matrix can have less than $n$ distinct eigenvalues
    \item A $n \times n$ matrix can have less than $n$ linear independent eigenvectors (also fewer then the number of distinct eigenvalues)
\end{itemize}

\defi{\textbf{Singular matrices} \label{def:singular_matrix}\\
A matrix is \textit{singular} if it has a zero eigenvalue.
\begin{equation*}
    Mx = 0x = 0
\end{equation*}
}

A singular matrix has linearly dependent columns (see Definition \ref{def:indipendency1}):
\begin{equation*}
    [M_1 ... M_{n-1} M_n] \begin{bmatrix}
           x_{1} \\
           x_{2} \\
           \vdots \\
           x_{n-1}
         \\
           x_{n}
         \end{bmatrix} = 0
\end{equation*}
\begin{equation*}
    M_1x_1+\hdots+M_{n-1}x_{n-1}+M_nx_n = 0
\end{equation*}
\begin{equation*}
    M_n = M_1 \frac{-x_1}{x_n}+\hdots+M_{n-1}\frac{-x_{n-1}}{x_n}
\end{equation*}

\defi{\textbf{Determinant} \label{def:determinant}\\
The \textit{determinant} $|M|$ of a $n \times n$ matrix $M$ is the product of its eigenvalues.
}

\textbf{Remark:} a matrix is \textit{invertible} if its determinant is not zero (i.e. it is not singular).

\defi{\textbf{Symmetric matrix} \label{def:symmetric_matrix}\\
A \textit{symmetric matrix} is a square matrix that is equal to its transpose.
\begin{equation*}
    A = A^T
\end{equation*}
Because equal matrices have equal dimensions, only square matrices can be symmetric. The entries of a symmetric matrix are symmetric with respect to the main diagonal. So if $a_{ij}$ denotes the entry in the $i$th row and $j$th column then:
\begin{equation*}
    a_{ji} = a_{ij}
\end{equation*}
for all indices $i$ and $j$.
}

If A is a real symmetric matrix, then any two eigenvectors ($x,z$) corresponding to distinct eigenvalues ($\lambda \neq \mu$) are orthogonal. $(x, \lambda)$ and $(z, \mu)$ IF $\lambda \neq \mu \Rightarrow <x,z>=0$. The proof of this preposition is reported in Figure \ref{fig:proof_eigen_symmetric}.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/Dim eigen symm matrix.pdf}
    \caption{Proof that $(x, \lambda)$ and $(z, \mu)$ IF $\lambda \neq \mu \Rightarrow <x,z>=0$.}
    \label{fig:proof_eigen_symmetric}
\end{figure}

\section{Eigen-decomposition}
The purpose of this section is to describe an iterative procedure to find eigenvalues and eigenvectors for a matrix $A$. Given $A$ we want to find $\lambda$ and $x$ values such that:

\begin{equation*}
    Ax=\lambda x
\end{equation*}

We multiply both sides of the equation by $x^T$ and we divide both sides of the equation by $x^T x$. We are not interested in the 0 vector, so the dot product $x^T x$ is not 0.

\begin{equation*}
    \frac{x^T A x}{x^T x} = \frac{x^T \lambda x}{x^T x}
\end{equation*}

For linearity we can move $\lambda$.

\begin{equation*}
    \frac{x^T A x}{x^T x} = \lambda \frac{x^T x}{x^T x} = \lambda
\end{equation*}

The fraction $\frac{x^T A x}{x^T x}$ is called \textit{Raleigh quotient}.

\paragraph{}

At this point we want to find the eigenvector which maximize the \textit{Raleigh quotient}. In this way we get the eigenvector ($x$) which corresponds to the maximal eigenvalue.

\begin{equation*}
    x = \mathit{argmax}_v \frac{v^T A v}{v^T v}
\end{equation*}

Now we normalize the obtained eigenvector $x$:
\begin{equation*}
    x \leftarrow \frac{x}{||x||}
\end{equation*}

Note that a normalized eigenvector is still an eigenvector.

In this way we have obtained the first eigenvalue and the first normalized eigenvector.

\paragraph{}

In order to compute the other eigenvectors and eigenvalues, we modify $A$ such that, if we repeat the previous procedure, we find another eigenvector. This modification is called \textit{deflation}.

\begin{equation*}
    \tilde{A} = A - \lambda x x^T
\end{equation*}

Note that $x x^T$ is not $x^T x$ which is a scalar (dot product). Indeed $x x^T$ is a matrix.

Deflation turns $x$ into an eigenvector for $\tilde{A}$ which has zero-eigenvalue:

\begin{equation*}
    \tilde{A}x = (A - \lambda x x^T) x = Ax - \lambda x x^T x
\end{equation*}
Since $x$ is normalized $x^T x = 1$, so:
\begin{equation*}
    \tilde{A}x = Ax - \lambda x
\end{equation*}
Since $\lambda$ and $x$ is an eigenvalue-eigenvector pair $Ax = \lambda x$, so:
\begin{equation*}
    \tilde{A}x = Ax - \lambda x = 0
\end{equation*}
We can conclude that $x$ is an eigenvector with 0 eigenvalue:
\begin{equation*}
    \tilde{A}x = 0x
\end{equation*}

\paragraph{}
At this point, we can repeat the maximization procedure on the deflated matrix. In particular, we maximize the Raleigh quotient $\frac{v^T \tilde{A} v}{v^T v}$. In this way we can find the maximal possible eigenvalue. For sure, we will not find $x$ since its corresponding eigenvalue is 0.
We can demonstrate that after the deflation operation, other eigenvalues-eigenvectors are unchanged:
\begin{equation*}
    \tilde{A}z = (A - \lambda x x^T) z = Az - \lambda x x^T z
\end{equation*}
As stated in Section \ref{sec:eigenvalues_eigenvectors}, in the case of symmetric matrices, eigenvectors with distinct eigenvalues are orthogonal, so:
\begin{equation*}
    \tilde{A}z = Az - \lambda x x^T z = Az
\end{equation*}
As a conclusion, for a symmetric matrix, dealing with $\tilde{A}$ is like working with $A$.

\paragraph{}
At the end of the day:
\begin{equation*}
    x' = \textit{argmax}_v \frac{v^T \tilde{A} v}{v^T v}
\end{equation*}
where $x'$ is the eigenvector with the second largest eigenvalue.
The procedure is iteratively repeated on the deflated matrix until solution is zero, which means that there are no more positive eigenvalues.
If the matrix has negative eigenvalues, I can recover them minimizing the Raleigh quotient instead of maximizing it.
At some point the solution of the iterative procedure will be 0 again. If the obtained set of eigenvalues is not full rank yet, we have to take into consideration the eigenvectors with zero eigenvalues. The eigenvectors with zero eigenvalues are obtained extending the obtained set to an orthonormal basis.

\paragraph{}
Eigen-decomposition allows to diagonalize a matrix.

\begin{itemize}
    \item Let $V = [v_1 \hdots v_n]$ be a matrix with orthonormal eigenvectors as columns
    
    \item Let $\Lambda$ be the diagonal matrix of corresponding eigenvalues
    
    \item A square simmetric matrix can be \textit{diagonalized} as:
\end{itemize}
\begin{equation}
    V^T A V = \Lambda
\end{equation}

\textbf{Remark:} a diagonalized matrix is much simpler to manage and has the same properties as the original one (e.g. same eigen-decomposition).

\vspace{5mm}

\textbf{Proof:}\\
From eigenvalue-eigenvector definition (Definition \ref{def:eigen}):
\begin{equation*}
    A[v_1 \hdots v_n] = [v_1 \hdots v_n]
  \begin{bmatrix}
    \lambda_{1} & & 0\\
    & \ddots & \\
    0 & & \lambda_{n}
  \end{bmatrix}
\end{equation*}
\begin{equation*}
    AV = V \Lambda
\end{equation*}

We multiply both sides by $V^{-1}$.
\begin{equation*}
    V^{-1}AV = V^{-1}V \Lambda
\end{equation*}

We have that $V^{-1}V = I$. What is more $V$ is a \textit{unitary matrix}, which means that its columns are orthonormal, for which: $V^{-1} = V^T$. As a consequence:
\begin{equation*}
    V^{T}AV = \Lambda
\end{equation*}

\defi{\textbf{Positive semi-definite matrix} \label{def:positive_semidefinite_matrix}\\
An $n \times n$ symmetric matrix $M$ is \textit{positive semi-definite} if all its eigenvalues are non-negative. Alternative sufficient and necessary conditions are:
\begin{itemize}
    \item for all $x \in \mathcal{R}^n$
\end{itemize}
\begin{equation*}
    x^T M x \geq 0
\end{equation*}
\begin{itemize}
    \item there exists a real matrix $B$ such that:
\end{itemize}
\begin{equation*}
    M = B^T B
\end{equation*}
}

\defi{\textbf{Positive definite matrix} \label{def:positive_definite_matrix}\\
An $n \times n$ symmetric matrix $M$ is \textit{positive definite} if all its eigenvalues are positive.
}

\section{Understanding eigendecomposition}

\subsection{Scaling transformation in standard basis}
Consider Figure \ref{fig:scalar_trans}.
\begin{itemize}
    \item Let $x_1 = [1,0]$, $x_2 = [0,1]$ be the standard orthonormal basis in $\mathcal{R}^2$
    \item Let $x = [x_1, x_2]$ be an arbitrary vector in $\mathcal{R}^2$
    \item A linear transformation is a \textit{scaling} transformation if it only stretches $x$ along its directions
\end{itemize}

In Figure \ref{fig:scalar_trans}, matrix $A$ encodes a scaling transformation.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{images/scalingVector.png}
    \caption{Scaling transformation in standard basis.}
    \label{fig:scalar_trans}
\end{figure}

\subsection{Scaling transformation in eigenbasis}
Consider Figure \ref{fig:eigenbasis_trans}.
\begin{itemize}
    \item Let $A$ be a non-scaling linear transformation in $\mathcal{R}^2$. Actually, transformation $Av$ is not a linear transformation, since $v$ has no components along the x\-axis.
    
    \item Let $\{v_1, v_2\}$ be an \textit{eigenbasis} for $A$. An eigenbasis is a basis in which every vector is an eigenvector.
    
    \item By representing vectors in $\mathcal{R}^2$ in terms of the $\{v_1,v_2\}$ basis (instead of the standard $\{x_1, x_2\}$), $A$ becomes a scaling transformation.
\end{itemize}

Eigendecomposition is useful since we can compute an eigenbasis for a matrix $A$ so that $A$ becomes a scaling transformation with respect to the coordinate system defined by the eigenbasis.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/eigenTrans.png}
    \caption{Scaling transformation in eigenbasis.}
    \label{fig:eigenbasis_trans}
\end{figure}

\section{Principal Component Analysis (PCA)}
Consider Figure \ref{fig:eigen_PCA}. $X$,$Y$ could be the height and the weight of a person. These two dimensions are correlated. As a result, if we collect some data from a population, we would obtain a distribution similar to the one represented on the left side of Figure \ref{fig:eigen_PCA}. On the other hand, the graphic on the right of Figure \ref{fig:eigen_PCA}, represents two quantities $P1, P2$ which are apparently uncorrelated. Indeed, the data spreads along the two dimensions.

\begin{itemize}
    \item Let $A$ be a data matrix with correlated coordinates.
    \item PCA is a linear transformation mapping data to a system of uncorrelated coordinates.
    \item It corresponds to fitting an ellipsoid to the data, whose axes are the coordinates of the new space.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/pca_eigen.png}
    \caption{Principal Component Analysis (PCA).}
    \label{fig:eigen_PCA}
\end{figure}

Given a dataset $X \in \mathcal{R}^{n \times d}$ with $n$ examples and $d$ dimensions.

\begin{enumerate}
    \item Compute the mean of the data ($X_i$ is the $i^{th}$ row vector of $X$):
    \begin{equation*}
        \Bar{x} = \frac{1}{n} \sum_{i=1}^n X_i
    \end{equation*}
    
    \item Center the data into the origin:
    \begin{equation*}
        X = X - \begin{bmatrix}
          \Bar{x} \\
          \vdots \\
          \Bar{x}
        \end{bmatrix}
    \end{equation*}
    
    \item Compute the data covariance:
    \begin{equation*}
        C = \frac{1}{n} X^T X
    \end{equation*}
    The covariance information highlights the correlations between the dimensions.
    
    \item Compute the (orthonormal) eigendecomposition of $C$:
    \begin{equation*}
        V^T C V = \Lambda
    \end{equation*}
    
    \item Use the set of eigen vectors as the new coordinate system:
    \begin{equation*}
        x' = V^{-1}x = V^Tx
    \end{equation*}
    ($V^{-1} = V^T$ as $V$ is unitary).
    In this new space (represented by the axis of the ellipse) the data are uncorrelated.
\end{enumerate}

\textbf{Remark:} this procedure assumes linear correlations (and Gaussian distributions).

\subsection{Dimensionality reduction}
Assume that we have computed the covariance matrix $C$ of a certain data distribution. What is more, we have also performed eigen value - eigen vector decomposition. At this point, it is possible to proof that each eigenvalue corresponds to the amount of variance in the direction of the corresponding eigenvector. The eigen vector which corresponds to the largest eigen values represents the directions of largest spread. The dimensions with the largest spread are the dimensions with the largest information. In order to perform \textit{dimensionality reduction} (e.g. visualization), we select only the $k$ eigenvectors with largest eigenvalues.\\
Suppose that we want to map the data in a $d$ dimensional space in a lower dimensional space with $k$ dimensions. To do this, we execute the same procedure described above for PCA, but this time we take into consideration only the first $k$ eigenvectors of the covariance matrix decomposition.
\begin{equation*}
    W = [v_1, \hdots, v_k]
\end{equation*}
Then, it is straightforward to map a point in the primary space to the new lower $k$-dimensional space (which retains the most information in terms of linear correlations):
\begin{equation*}
    x' = W^T x
\end{equation*}

Dimensionality reduction could be a valid pre-computation before applying other machine learning algorithms which perhaps performs poorly on high dimensional scenarios.